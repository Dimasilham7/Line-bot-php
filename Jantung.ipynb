{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jantung.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEw+Z1RCrpoH/jVzfjvD4/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimasilham7/Line-bot-php/blob/master/Jantung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUk3Ei1cr6fX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "ba9d84b6-8f68-47cc-9acd-175d2735c992"
      },
      "source": [
        "# Install required libs  \n",
        "# NOTE: Run this one code, then restart this runtime and run again for next all... (PENTING!!!) \n",
        "\n",
        "### please update Albumentations to version>=0.3.0 for `Lambda` transform support\n",
        "!pip install -U --pre segmentation-models --user"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: segmentation-models in /root/.local/lib/python3.6/site-packages (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: efficientnet==1.0.0 in /root/.local/lib/python3.6/site-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: image-classifiers==1.0.0 in /root/.local/lib/python3.6/site-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxP4KFpIs1r7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "dcad3834-fe08-4d83-ff4c-76f59c6320ce"
      },
      "source": [
        "!pip show segmentation_models\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: segmentation-models\n",
            "Version: 1.0.1\n",
            "Summary: Image segmentation models with pre-trained backbones with Keras.\n",
            "Home-page: https://github.com/qubvel/segmentation_models\n",
            "Author: Pavel Yakubovskiy\n",
            "Author-email: qubvel@gmail.com\n",
            "License: MIT\n",
            "Location: /root/.local/lib/python3.6/site-packages\n",
            "Requires: efficientnet, image-classifiers, keras-applications\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LamlioVatAJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b993b7c-6650-493a-95e1-059d12c8cd90"
      },
      "source": [
        "## Imports libs\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import cv2\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuPgm7m2tCeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "335400fd-2213-4167-9fbf-b368e16c44b5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0u2JJtItLtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_link = '/content/drive/My Drive/'\n",
        "my_link = drive_link+'SKRIPSI/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuqVrsQ5taPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d2bf7b5-4d0b-4c97-d3ec-874691505dcf"
      },
      "source": [
        "DATA_DIR = my_link+'dataset/'\n",
        "\n",
        "# load repo with data if it is not exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "  print('Dataset not exist...')\n",
        "else:\n",
        "  print('Dataset Exist...')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Exist...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_sDMlInthqW",
        "colab_type": "text"
      },
      "source": [
        "### Data Generator\n",
        "###### ``Data Generator adalah kelas generator dataset yang dibungkus per batch sesuai dengan ukuran batch, ``\n",
        "###### ``misal (batch_size, pixel_width, pixel_height, channel_size) = (8,128,128,3)``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrSh3vGxtcBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGen(keras.utils.Sequence):\n",
        "    def __init__(self, ids, path, batch_size=8, image_size=256, augmentation=None, preprocessing=None): \n",
        "        self.ids = ids\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "        \n",
        "    def __load__(self,id_name):\n",
        "        ## Path\n",
        "        image_path = os.path.join(self.path, \"images\", id_name) + \".png\"\n",
        "        mask_path = os.path.join(self.path, \"masks\", id_name) + \".png\"\n",
        "        \n",
        "        ## Reading Image\n",
        "        image = cv2.imread(image_path, 1)\n",
        "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
        "        \n",
        "        ## Reading Masks\n",
        "        mask = cv2.imread(mask_path, -1)\n",
        "        mask = cv2.resize(mask, (self.image_size, self.image_size))\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "            \n",
        "        ## Normalizaing \n",
        "        image = image/255.0\n",
        "        mask = mask/255.0\n",
        "        \n",
        "        return image, mask\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ## Jumlah Batch Terakhir Disesuaikan Sisa Data Yang Ada\n",
        "        if(index+1)*self.batch_size > len(self.ids):\n",
        "            self.batch_size = len(self.ids) - index*self.batch_size\n",
        "        \n",
        "        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        \n",
        "        image = []\n",
        "        mask  = []\n",
        "        \n",
        "        ## Load Data Dengan Fungsi __load__\n",
        "        for id_name in files_batch:\n",
        "            _img, _mask = self.__load__(id_name)  \n",
        "            image.append(_img)\n",
        "            mask.append(_mask)\n",
        "            \n",
        "        image = np.array(image)\n",
        "        mask  = np.array(mask)\n",
        "\n",
        "        return image, mask\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "    \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.ids)/float(self.batch_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAUiPFrCtkQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5811da56-7a71-4c8a-d820-c23154149925"
      },
      "source": [
        "# Data jantung generate ids\n",
        "\n",
        "train_path = DATA_DIR\n",
        "jantung_path = '/content/drive/My Drive/SKRIPSI/dataset/jantung_idx.txt' # daftar nama data jantung ada disini\n",
        "load_jt_ids = []\n",
        "\n",
        "# Load idx dataset jantung\n",
        "with open(jantung_path, 'rb') as handle: \n",
        "    load_jt_ids = [str(line.rstrip())[2:-1] for line in handle]\n",
        "\n",
        "# Membagi dataset menjadi train, valid, test = 70%, 20%, 10% \n",
        "train_jt_ids = load_jt_ids[:int(5011*0.70)+1]\n",
        "valid_jt_ids = load_jt_ids[int(5011*0.70)+1:-int(5011*0.10)]\n",
        "test_jt_ids = load_jt_ids[-int(5011*0.10):]\n",
        "\n",
        "print('Jumlah data jantung (train, valid, test) = (',len(train_jt_ids), ',', len(valid_jt_ids), ',', len(test_jt_ids),').')\n",
        "print('Total dataset: ',len(train_jt_ids)+len(valid_jt_ids)+len(test_jt_ids))\n",
        "print('Contoh data:',load_jt_ids[-1])\n",
        "        "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jumlah data jantung (train, valid, test) = ( 3508 , 1002 , 501 ).\n",
            "Total dataset:  5011\n",
            "Contoh data: sol_033_z_pos_010_t_pos_020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaJ19qUMtxNo",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_QKcVPBtuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############____________________________________________________________############\n",
        "\n",
        "# Definisikan model dan parameter yang diinginkan... \n",
        "# Definisi parameter ini akan digenerate otomatis oleh fungsi-fungsi dibawahnya\n",
        "\n",
        "model_path = 'unet_mobilenet_jt_0.001_15_256_yes'\n",
        "\n",
        "############____________________________________________________________############"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPwLMQzIuCSL",
        "colab_type": "text"
      },
      "source": [
        "Definisikan model_path: ``{decoder}_{encoder}_jt_{learning_rate}_{epochs}_{image_size}_{pretrained_imagenet?}``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyiTIfSjuCO-",
        "colab_type": "text"
      },
      "source": [
        "**Berikut definisi encoder di sm-model:**\n",
        "##### ``VGG``\t: 'vgg16', 'vgg19'\n",
        "##### ``ResNet``\t: 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152'\n",
        "##### ``SENet154``\t: \t'senet154'\n",
        "##### ``DenseNet``\t: \t'densenet121', 'densenet169', 'densenet201'\n",
        "##### ``Inception``\t: \t'inceptionv3', 'inceptionresnetv2'\n",
        "##### ``MobileNet``\t: \t'mobilenet', 'mobilenetv2'\n",
        "##### ``EfficientNet``\t: \t'efficientnetb0', 'efficientnetb1', 'efficientnetb2', 'efficientnetb3', 'efficientnetb4', 'efficientnetb5' efficientnetb6', efficientnetb7'\n",
        "\n",
        "**Berikut definisi decoder di sm-model:**\n",
        "- unet\n",
        "- linknet\n",
        "- pspnet\n",
        "- fpn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0oX7FraETZA",
        "colab_type": "text"
      },
      "source": [
        "#### Fungsi dan Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2qlazo7t_1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Posisi direktori model\n",
        "save_dir =  [\n",
        "              '/content/drive/My Drive/SKRIPSI/models/',\n",
        "            ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4DYhjnJEXUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fungsi untuk mengolah history\n",
        "import json, codecs, pickle\n",
        "\n",
        "def saveHist(path, history):\n",
        "    with open(path, 'wb') as handle: # saving the history of the model\n",
        "        pickle.dump(history, handle)\n",
        "\n",
        "def loadHist(path):\n",
        "    n = {} # set history to empty\n",
        "    if os.path.exists(path): # reload history if it exists\n",
        "        with open(path, 'rb') as handle: # loading old history \n",
        "            n = pickle.load(handle)\n",
        "    return n\n",
        "\n",
        "def appendHist(h1, h2):\n",
        "    if h1 == {}:\n",
        "        return h2\n",
        "    else:\n",
        "        dest = {}\n",
        "        for key, value in h1.items():\n",
        "            dest[key] = value + h2[key]\n",
        "        return dest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1CIRmeVEZqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import segmentation_models as sm\n",
        "\n",
        "# Mengambil Model Yang Sudah Ada\n",
        "def take_model(model_path):\n",
        "    print('Melanjutkan: ',model_path)\n",
        "    md_ = model_path.split('_')\n",
        "    lr = float(md_[3])\n",
        "    epochs = int(md_[4])\n",
        "    image_size = int(md_[5])\n",
        "    pretrained = md_[6]\n",
        "    batch_size = 8\n",
        "    print('lr:',lr,', epochs:',epochs,', im_size:',image_size,', prt: '+pretrained)\n",
        "    # optim, loss, metric harus sesuai dengan model yang ditraining sebelumnya\n",
        "    optim = keras.optimizers.Adam(lr)\n",
        "    total_loss = sm.losses.JaccardLoss()\n",
        "    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), sm.metrics.Dice]\n",
        "\n",
        "    model = keras.models.load_model(save_dir[0]+model_path+'/'+model_path+'.h5', custom_objects={'jaccard_loss': total_loss, 'iou_score':metrics[0], 'f1-score':metrics[1]})\n",
        "\n",
        "    return model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim\n",
        "\n",
        "# Mengatur Dataset\n",
        "def set_data(train_ids,test_ids,valid_ids):\n",
        "    train_gen = DataGen(train_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
        "    test_gen = DataGen(test_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
        "    valid_gen = DataGen(valid_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
        "\n",
        "    train_steps = len(train_ids)//batch_size\n",
        "    test_steps = len(test_ids)//batch_size\n",
        "    valid_steps = len(valid_ids)//batch_size\n",
        "\n",
        "    return train_gen,test_gen,valid_gen,train_steps,test_steps,valid_steps\n",
        "\n",
        "# Melanjutkan Training\n",
        "def cont_training(model):\n",
        "    print('Lanjutkan Training... '+model.name)   \n",
        "    path = save_dir[0]+model.name+'/'+model.name\n",
        "    old_hist = loadHist(path+'_history.bin')\n",
        "    initial_epoch = len(old_hist['loss'])\n",
        "    print('Dari epochs',initial_epoch)\n",
        "    if initial_epoch>=500 or initial_epoch==50 or initial_epoch==250:\n",
        "        print(\"Batal Training, Model Sudah Memenuhi Epochs \"+str(initial_epoch))\n",
        "    else:\n",
        "        # define callbacks for learning rate scheduling and best checkpoints saving\n",
        "        callbacks = [\n",
        "            keras.callbacks.ModelCheckpoint(path+'_best_weights.h5', monitor='val_iou_score', save_weights_only=True, save_best_only=True, mode='max'),\n",
        "        ]\n",
        "        saveHist(path+'_history_temp_'+str(initial_epoch)+'.bin', old_hist)\n",
        "        model.save(path+'_temp_'+str(initial_epoch)+'.h5')\n",
        "        model.fit_generator(train_gen, \n",
        "                            validation_data=valid_gen, \n",
        "                            steps_per_epoch=train_steps, \n",
        "                            validation_steps=valid_steps, \n",
        "                            epochs=initial_epoch+epochs,\n",
        "                            initial_epoch=initial_epoch,\n",
        "                            callbacks=callbacks)\n",
        "        print('Menyimpan Model...')    \n",
        "        model.save(path+'.h5')\n",
        "        history = model.history\n",
        "        model.history.history = appendHist(old_hist, history.history)\n",
        "        saveHist(path+'_history.bin', model.history.history)\n",
        "        if initial_epoch>=400:\n",
        "            train_vis(model.history.history, path, doing=\"save\")\n",
        "        print('Berhasil Menyimpan Model')\n",
        "\n",
        "# Membuat Model SM Baru\n",
        "def take_sm(model_path):\n",
        "    print('Memulai SM-Model: ',model_path)\n",
        "    try:\n",
        "        # apakah model sudah pernah dibuat?\n",
        "        os.mkdir(save_dir[0]+model_path)\n",
        "    except:\n",
        "        print('Overwrite '+model_path)\n",
        "    md_ = model_path.split('_')\n",
        "    lr = float(md_[3])\n",
        "    epochs = int(md_[4])\n",
        "    image_size = int(md_[5])\n",
        "    pretrained = md_[6]\n",
        "    batch_size = 8\n",
        "    print('lr:',lr,', epochs:',epochs,', im_size:',image_size,', prt: '+pretrained)\n",
        "    # optimizer manual setting \n",
        "    optim = keras.optimizers.Adam(lr)\n",
        "    # loss function manual setting\n",
        "    total_loss = sm.losses.JaccardLoss()\n",
        "    # metric evaluation manual setting\n",
        "    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
        "    if pretrained=='yes': encoder_weights = 'imagenet'\n",
        "    else: encoder_weights = None\n",
        "    if md_[0] is 'unet':\n",
        "        model = sm.Unet(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    elif md_[0] is 'linknet':\n",
        "        model = sm.Linknet(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    elif md_[0] is 'pspnet':\n",
        "        model = sm.PSPNet(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    elif md_[0] is 'fpn':\n",
        "        model = sm.FPN(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    else: \n",
        "        model = sm.Unet(md_[1], classes=1, activation='sigmoid', input_shape=(image_size,image_size,3), encoder_weights=encoder_weights) \n",
        "    model.compile(optim, total_loss, metrics)\n",
        "    # auto set model name with model_path\n",
        "    model.name = model_path\n",
        "    return model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aowmVe7GEiLY",
        "colab_type": "text"
      },
      "source": [
        "#### Persiapan Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyWBEAuXEfBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########____________________________________________ Pengaturan Model dan Dataset ____________________________________________##########\n",
        "\n",
        "### Untuk membuat model sm baru, menggunakan fungsi take_sm() ###\n",
        "model, lr, epochs, image_size, pretrained, batch_size, metrics, total_loss, optim = take_sm(model_path)\n",
        "train_gen, test_gen, valid_gen, train_steps, test_steps, valid_steps = set_data(train_jt_ids,test_jt_ids,valid_jt_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hifggT38EiHN",
        "colab_type": "text"
      },
      "source": [
        "#### Fungsi Training Testing dan Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yEBaGy3EZoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "import json, codecs, pickle\n",
        "import pandas as pd\n",
        "\n",
        "# helper function for training visualization\n",
        "def train_vis(history, model_save_dir, doing=\"show\"):\n",
        "    # Plot training & validation iou_score values\n",
        "    plt.figure(figsize=(30, 5))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(history['iou_score'])\n",
        "    plt.plot(history['val_iou_score'])\n",
        "    plt.title('Model iou_score')\n",
        "    plt.ylabel('iou_score')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(122)\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    if (doing==\"save\"):\n",
        "        plt.savefig(model_save_dir+'_plot.png')\n",
        "        print(\"Success Saving Plot\")\n",
        "        plt.clf()\n",
        "    else: \n",
        "        plt.show()\n",
        "\n",
        "# helper function for training \n",
        "def train_fit(model_, epochs=epochs, pretrain=False):\n",
        "    print(\"Training for \"+model_.name)\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(save_dir[0]+\"{}/{}\".format(model_.name,model_.name)+'_best_weights.h5', monitor='val_iou_score', save_weights_only=True, save_best_only=True, mode='max'),\n",
        "    ]\n",
        "    model_.fit_generator( train_gen, \n",
        "                          validation_data=valid_gen, \n",
        "                          steps_per_epoch=train_steps, \n",
        "                          validation_steps=valid_steps, \n",
        "                          epochs=epochs,\n",
        "                          callbacks=callbacks)\n",
        "        \n",
        "    if pretrain:\n",
        "        return 0\n",
        "        \n",
        "    try:\n",
        "        os.mkdir(save_dir[0]+model_.name)\n",
        "    except FileExistsError:\n",
        "        print('Directory not created, '+model_.name+' was exist!')\n",
        "\n",
        "    model_save_dir = save_dir[0]+model_.name+'/'+model_.name\n",
        "\n",
        "    plot_model(model_, show_shapes=True, to_file=model_save_dir+'_architecture.png')\n",
        "\n",
        "    train_vis(model_.history.history, model_save_dir, doing=\"save\")\n",
        "\n",
        "    with open(model_save_dir+'_history.bin', 'wb') as handle:\n",
        "        pickle.dump(model_.history.history, handle)\n",
        "    \n",
        "    model_.save(model_save_dir+'.h5')\n",
        "    print(\"Success Saving Model\")\n",
        "\n",
        "# helper function for testing \n",
        "def test_eval(model_):\n",
        "    i_=0\n",
        "    list_of_test = []\n",
        "    print(\"Testing for \"+model_.name)\n",
        "    scores = model_.evaluate_generator(test_gen)\n",
        "    list_of_test.append(\"Loss: {:.5}\".format(scores[0]))\n",
        "    print(list_of_test[i_])\n",
        "    for metric, value in zip(metrics, scores[1:]):\n",
        "        i_ += 1\n",
        "        list_of_test.append(\"mean {}: {:.5}\".format(metric.__name__, value))\n",
        "        print(list_of_test[i_])\n",
        "\n",
        "    model_save_dir = save_dir[0]+model_.name+'/'+model_.name\n",
        "    \n",
        "    with open(model_save_dir+'_scores.txt', 'w') as f:\n",
        "        for item in list_of_test:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "# helper function for data visualization\n",
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "    \n",
        "# helper function for data visualization    \n",
        "def denormalize(x):\n",
        "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
        "    x_max = np.percentile(x, 98)\n",
        "    x_min = np.percentile(x, 2)    \n",
        "    x = (x - x_min) / (x_max - x_min)\n",
        "    x = x.clip(0, 1)\n",
        "    return x\n",
        "\n",
        "# helper function for test result and visualization\n",
        "def test_vis(model, test_ids=test_jt_ids, evals=False):\n",
        "    # new definition test data \n",
        "    test_dataset = DataGen(test_ids, train_path, image_size=image_size, batch_size=1)\n",
        "    \n",
        "    loss_ = []\n",
        "    ious_ = []\n",
        "    f1s_ = []\n",
        "    task_ = ''\n",
        "\n",
        "    if evals:\n",
        "        jml = len(test_dataset)\n",
        "        ids = range(0,jml)\n",
        "        task_ = 'Sukses mengevaluasi model '+model.name+', dengan jumlah data_test: '+str(jml)\n",
        "    else:\n",
        "        #n = 5\n",
        "        #ids = np.random.choice(np.arange(len(test_dataset)), size=n)\n",
        "        jml = len(test_dataset)\n",
        "        ids = range(0,jml)\n",
        "\n",
        "    # visualize\n",
        "    for i in ids:\n",
        "        image, gt_mask = test_dataset[i]\n",
        "        image = np.expand_dims(image[0], axis=0)\n",
        "        pr_mask = model.predict(image).round()\n",
        "      \n",
        "        if not evals:\n",
        "            visualize(\n",
        "                image=denormalize(image.squeeze()),\n",
        "                gt_mask=gt_mask[0][..., 0].squeeze(),\n",
        "                pr_mask=pr_mask[0][..., 0].squeeze(),\n",
        "            )\n",
        "        else:\n",
        "            scores = model.evaluate(image,gt_mask)\n",
        "            \n",
        "            if (i+1) % 10 == 0 :\n",
        "                print('['+str(i+1)+'/'+str(jml)+'] --- dari progres ---')\n",
        "            \n",
        "            loss_.append(scores[0])\n",
        "            ious_.append(scores[1])\n",
        "            f1s_.append(scores[2])\n",
        "\n",
        "    if evals:\n",
        "        df = pd.DataFrame({'id': test_ids[:jml], 'loss': loss_, 'iou-score': ious_, 'f1-score': f1s_})\n",
        "        df.to_csv(save_dir[0]+model.name+'/'+model.name+'_eval_test.csv',index=False)\n",
        "    print()\n",
        "    print(task_)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmMQUk1JEtJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "round(0.1284*100,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_NllGrQEz3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load weights in the best scores for each overall epoh\n",
        "# model.load_weights('/content/drive/My Drive/PENELITIAN/models/unet_mobilenet_17a_0.001_100_256_yes/unet_mobilenet_17a_0.001_100_256_yes_best_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w1kwsriE3V3",
        "colab_type": "text"
      },
      "source": [
        "#### Train The Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrGfgjn-E4zO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Melanjutkan Training\n",
        "##cont_training(model)\n",
        "\n",
        "# Training Model Baru\n",
        "train_fit(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6D_A7OhE7HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.load_model('/content/drive/My Drive/SKRIPSI/models/unet_mobilenet_jt_0.001_15_256_yes/unet_mobilenet_jt_0.001_15_256_yes.h5',custom_objects={'jaccard_loss': total_loss, 'iou_score':metrics[0], 'f1-score':metrics[1]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C8x4l7nE9PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/drive/My Drive/SKRIPSI/models/unet_mobilenet_jt_0.001_15_256_yes/unet_mobilenet_jt_0.001_15_256_yes.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECywxCDBE_9o",
        "colab_type": "text"
      },
      "source": [
        "#### Test The Models \n",
        "`*(Tidak perlu dijalankan jika masih checkpoint)*`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCoSVSf3E9NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eval(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ZsB9CJE7KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_vis(model, test_ids=test_jt_ids, evals=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1rE7F94FGq_",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluasi Gambar\n",
        "``Print kondisi setiap channel pada setiap layer untuk gambar yang akan didefinisikan``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SFN4teTFJWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_csvs = save_dir[0]+model.name+\"/\"+model.name+\"_eval_test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCuAikiYFMHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.read_csv(load_csvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1MKT3sHFOmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfcsv = pd.read_csv(load_csvs).sort_values('iou-score')\n",
        "nilai_iou = []\n",
        "# dfcsv\n",
        "for niou in dfcsv['iou-score']:\n",
        "     nilai_iou.append(int(round(niou*100,0)))\n",
        "nilai_iou[0] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiLLT3taFQel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index_ = list(pd.DataFrame(nilai_iou)[0].value_counts().index)\n",
        "values_ = list(pd.DataFrame(nilai_iou)[0].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsGTlwJCFQl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = index_\n",
        "values = values_\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.bar(names, values)\n",
        "plt.xticks(list(range(0,101,2)))\n",
        "plt.yticks(list(range(0,31,2)))\n",
        "plt.xlabel('Skor IoU (Pembulatan 0 angka di belakang koma)')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.grid()\n",
        "plt.suptitle('Sebaran Skor IoU pada 600 Data Uji')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtFgmU0_FMPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = index_\n",
        "values = values_\n",
        "\n",
        "plt.figure(figsize=(3, 15))\n",
        "\n",
        "plt.boxplot(nilai_iou)\n",
        "plt.ylabel('Skor IoU (%)')\n",
        "plt.yticks(list(range(0,101,2)))\n",
        "plt.grid()\n",
        "plt.suptitle('Sebaran Skor IoU pada 600 Data Uji')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edZNfgInFUsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfcsv = pd.read_csv(load_csvs).sort_values('iou-score')\n",
        "print(dfcsv[dfcsv['iou-score']<0.1].count()[0])\n",
        "print(dfcsv[dfcsv['iou-score']>=0.1][dfcsv['iou-score']<=0.5].count()[0])\n",
        "print(dfcsv[dfcsv['iou-score']>0.5][dfcsv['iou-score']<=0.85].count()[0])\n",
        "print(dfcsv[dfcsv['iou-score']>0.85][dfcsv['iou-score']<=0.95].count()[0])\n",
        "print(dfcsv[dfcsv['iou-score']>0.95].count()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-U0Eyb5FXFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.read_csv(load_csvs)['iou-score'].sum()/600"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-Mc_EwyFZj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Hasil ini digunakan pada model yang dilatih dengan batch_size 8')\n",
        "print()\n",
        "test_dataset = DataGen(test_jt_ids, train_path, image_size=image_size, batch_size=8)\n",
        "hasils = model.evaluate_generator(test_dataset)\n",
        "print(\"Hasil pengujian dengan batch_size = 8 (sesuai ukuran pelatihan)\")\n",
        "print(\"Loss: \",hasils[0])\n",
        "print(\"IoU-Score: \",hasils[1])\n",
        "print(\"F1-Score: \",hasils[2])\n",
        "print()\n",
        "\n",
        "test_dataset = DataGen(test_jt_ids, train_path, image_size=image_size, batch_size=1)\n",
        "hasils = model.evaluate_generator(test_dataset)\n",
        "print(\"Hasil pengujian dengan batch_size = 1 (satu-per-satu gambar)\")\n",
        "print(\"Loss: \",hasils[0])\n",
        "print(\"IoU-Score: \",hasils[1])\n",
        "print(\"F1-Score: \",hasils[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFlQPlQ6FfWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfcsv = pd.read_csv(load_csvs).sort_values('iou-score')\n",
        "test_id1 = list(dfcsv['id'][dfcsv['iou-score']<0.1])\n",
        "test_id2 = list(dfcsv['id'][dfcsv['iou-score']>=0.1][dfcsv['iou-score']<=0.5])\n",
        "test_id3 = list(dfcsv['id'][dfcsv['iou-score']>0.5][dfcsv['iou-score']<=0.85])\n",
        "test_id4 = list(dfcsv['id'][dfcsv['iou-score']>0.85][dfcsv['iou-score']<=0.95])\n",
        "test_id5 = list(dfcsv['id'][dfcsv['iou-score']>0.95])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uyQWnUKFiW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fungsi_vis(i,test_id1):\n",
        "  test_dataset = DataGen(test_id1, train_path, image_size=image_size, batch_size=1)\n",
        "  plt.figure(figsize=(16, 5))\n",
        "  image, gt_mask = test_dataset[i]\n",
        "  image = np.expand_dims(image[0], axis=0)\n",
        "  pr_mask = model.predict(image).round()\n",
        "  scores = model.evaluate(image,gt_mask)\n",
        "  b,g,r = cv2.split(image[0])\n",
        "  rgb_img = cv2.merge([r,g,b])\n",
        "  # imagess = np.expand_dims(rgb_img, axis=0)\n",
        "\n",
        "  contours1, hierarchy1 = cv2.findContours(np.array(gt_mask[0], dtype=np.uint8).squeeze(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
        "  largest_contour1 = []\n",
        "  largest_area = 0\n",
        "  for contour in contours1:\n",
        "      area = cv2.contourArea(contour)\n",
        "      if area > largest_area:\n",
        "          largest_area = area\n",
        "          largest_contour1 = contour\n",
        "  cv2.drawContours(rgb_img, [largest_contour1], -2, (1, 1, 0), 1)\n",
        "\n",
        "  contours2, hierarchy2 = cv2.findContours(np.array(pr_mask, dtype=np.uint8).squeeze(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
        "  largest_contour2 = []\n",
        "  largest_area = 0\n",
        "  for contour in contours2:\n",
        "      area = cv2.contourArea(contour)\n",
        "      if area > largest_area:\n",
        "          largest_area = area\n",
        "          largest_contour2 = contour\n",
        "  cv2.drawContours(rgb_img, [largest_contour2], -1, (0, 1, 1), 1)\n",
        "  ious = str(scores[1]*100)[:str(scores[1]*100).find(\".\")+3] + '%'\n",
        "  titles = test_id1[i]+\" (\"+ious+\")\"\n",
        "  plt.title(titles)\n",
        "  plt.imshow(denormalize(rgb_img.squeeze()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnxp_he8FmMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(['sol_002_z_pos_011_t_pos_006','sol_001_z_pos_002_t_pos_001','sol_001_z_pos_002_t_pos_001'])):\n",
        "  fungsi_vis(i,['sol_002_z_pos_011_t_pos_006','sol_001_z_pos_002_t_pos_001','sol_001_z_pos_002_t_pos_001'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4bzRrh6Fj8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id3[50:100])):\n",
        "  fungsi_vis(i,test_id3[50:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNOScWPUFohC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id3[-70:-20])):\n",
        "  fungsi_vis(i,test_id3[-70:-20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaEncNP_FqEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id4[:20])):\n",
        "  fungsi_vis(i,test_id4[:20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpa3BQWeFqLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id4[-120:-80])):\n",
        "  fungsi_vis(i,test_id4[-120:-80])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-bQIxyVFtgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_id5)):\n",
        "  fungsi_vis(i,test_id5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8F7OoncFwSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########____________________________________________________________________________##########\n",
        "\n",
        "# Gambar yang akan dievaluasi (Definisikan!!!)\n",
        "jenisimg = ['own','97','90','85','70','50','20','0,01','0,001'] # Kategori Berdasarkan Mean-IoU-Scores\n",
        "eval_ids = ['sol_001_z_pos_002_t_pos_001','sol_001_z_pos_002_t_pos_015','sol_001_z_pos_002_t_pos_016','sol_001_z_pos_002_t_pos_017','sol_001_z_pos_002_t_pos_019','sol_001_z_pos_002_t_pos_020','sol_001_z_pos_003_t_pos_002','sol_001_z_pos_003_t_pos_003','sol_001_z_pos_003_t_pos_004']\n",
        "\n",
        "link_fldr = '/content/drive/My Drive/SKRIPSI/visualisasi/'\n",
        "\n",
        "##########____________________________________________________________________________##########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVjDPD19FodR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_data = DataGen(eval_ids, train_path, image_size=image_size, batch_size=1)\n",
        "x,y = eval_data.__getitem__(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X6f34J8F0mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# Print Image Input Layers\n",
        "def printInput(x,y,i):\n",
        "    print(x[0].shape)\n",
        "    b,g,r = cv2.split(x[0])\n",
        "    rgb_img = cv2.merge([r,g,b]) \n",
        "    pr = model.predict(x)\n",
        "    fig, axs = plt.subplots(1, 6, figsize=(30,5))\n",
        "    fig.suptitle('(0) Layer Input')\n",
        "    axs[0].imshow(rgb_img) # Image\n",
        "    axs[0].set_title('Image '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[1].imshow(x[0][:,:,0]) # Blue\n",
        "    axs[1].set_title('Blue channel '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[2].imshow(x[0][:,:,1]) # Green\n",
        "    axs[2].set_title('Green channel '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[3].imshow(x[0][:,:,2]) # Red\n",
        "    axs[3].set_title('Red channel '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[4].imshow(y[0][:,:,0]) # Mask\n",
        "    axs[4].set_title('Mask '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "    axs[5].imshow(pr[0][:,:,0]) # Predict\n",
        "    axs[5].set_title('Predict '+eval_ids[i]+' with score: '+jenisimg[i]+'%')\n",
        "\n",
        "    fig.savefig(link_fldr+eval_ids[i]+'/(0) Layer Input.png')\n",
        "    print(\"Success save layer input...\")\n",
        "\n",
        "# Pembagi Plot Size \n",
        "def rec(size_,pem=2,pen=2):\n",
        "    hs = size_/4\n",
        "    if size_ == 1: return pem-pem, pen-pen\n",
        "    if hs == 1: return pem, pen\n",
        "    elif hs % 4 != 0: return pem, pen*2\n",
        "    else: return rec(hs,pem*2,pen*2)\n",
        "\n",
        "# Print Image Every Layers\n",
        "def printLayer(model,x,k,first_,last_):\n",
        "    for i in range(first_,last_):\n",
        "        print('('+str(i)+') Layer '+model.layers[i].name+'.png')\n",
        "        if 'concat' in model.layers[i].name or 'pad' in model.layers[i].name:\n",
        "            print('Skip layer '+model.layers[i].name)\n",
        "        else:\n",
        "            get_output = K.function([model.layers[0].input],\n",
        "                                    [model.layers[i].output])\n",
        "            \n",
        "            layer_output = get_output([x])[0]\n",
        "\n",
        "            pem, pen = rec(len(layer_output[0][0][0]))\n",
        "\n",
        "            if pem == 0:\n",
        "                plt.imshow(layer_output[0][...,0])\n",
        "                plt.title(\"Ch_1\")\n",
        "                # Save Subplot.\n",
        "                plt.savefig(link_fldr+eval_ids[k]+'/('+str(i)+') Layer '+model.layers[i].name+'.png')\n",
        "            else:\n",
        "\n",
        "                # Create a Subplot.\n",
        "                fig, axs = plt.subplots(pen, pem, figsize=(32,32*pen//pem))\n",
        "                fig.suptitle('('+str(i)+') Layer '+model.layers[i].name)\n",
        "                \n",
        "                for ii in range(pen):\n",
        "                    for j in range(pem):\n",
        "                        axs[ii, j].imshow(layer_output[0][...,(ii*pem)+j])\n",
        "                        axs[ii, j].axis('off')\n",
        "                        axs[ii, j].set_title(\"Ch_\"+str((ii*pem)+j+1))\n",
        "                \n",
        "                # Save Subplot.\n",
        "                fig.savefig(link_fldr+eval_ids[k]+'/('+str(i)+') Layer '+model.layers[i].name+'.png')\n",
        "\n",
        "            print(\"Success save layer \"+model.layers[i].name+\"...\")\n",
        "            print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXyfa40EFzA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definisikan lt berdasarkan urutan gambar (dibagi supaya ram tidak penuh)\n",
        "lt = 0\n",
        "\n",
        "print('Percobaan kurang:',len(eval_ids)-lt-1,'gambar')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjz-VSj8F2ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute 1/2 data\n",
        "for i in range(lt,lt+1):\n",
        "    print(eval_ids[i])\n",
        "    try:\n",
        "        os.mkdir(link_fldr+eval_ids[i]+'/')\n",
        "    except FileExistsError:\n",
        "        print('Directory not created, '+eval_ids[i]+' was exist!')\n",
        "    x_,y_ = eval_data.__getitem__(i)\n",
        "    printInput(x_,y_,i)\n",
        "    printLayer(model,x_,i,1,len(model.layers)//2)\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YiiBW1ZF6Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Execute 2/2 data\n",
        "for i in range(lt,lt+1):\n",
        "   print(eval_ids[i])\n",
        "    try:\n",
        "        os.mkdir(link_fldr+eval_ids[i]+'/')\n",
        "    except FileExistsError:\n",
        "        print('Directory not created, '+eval_ids[i]+' was exist!')\n",
        "    x_,y_ = eval_data.__getitem__(i)\n",
        "    printInput(x_,y_,i)\n",
        "    printLayer(model,x_,i,len(model.layers)//2,len(model.layers))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow7kyJVhF9Fq",
        "colab_type": "text"
      },
      "source": [
        "#### Finish !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRp82VzGGCC2",
        "colab_type": "text"
      },
      "source": [
        "![alt text]Finish !... Semua File Akan Otomatis Replace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-w8mZYiGFVE",
        "colab_type": "text"
      },
      "source": [
        "**Penting !...** *``Cara memastikan model sudah 500 epochs yaitu dengan melihat \n",
        "\n",
        "---\n",
        "\n",
        "size pada file \"history.bin\", ketika sudah berukuran **\"91KB\"** artinya model tersebut sudah 500 epochs...*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb01g63PF2tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTXROCWtF9nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}